{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src=\"https://bdaaosu.org/img/Logo.png\" width=\"60%\"/>\n",
    "\n",
    "# Text Classification Workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A note from the editor:\n",
    "\n",
    "If you are new to Python, this may seem like a lot, _all at once_. Don't worry if you don't understand everything, right now! If you do not understand what a function does, simply run the following code:\n",
    "\n",
    "`help(<FUNCTION NAME, WITHOUT PARENTHESIS>)`\n",
    "\n",
    "And you should get some helpful documentation back!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run these lines if you do not have sklearn and/or pandas installed!\n",
    "!pip install sklearn\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Helper function - Run this, but don't necessarily worry about how it works!\n",
    "def most_associated_words(tfidf, features, labels):\n",
    "    \"\"\"\n",
    "    Print the most associated unigrams and bigrams for the given\n",
    "    feature matrix and class labels.\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    tfidf : TfidfVectorizer object from sklearn.feature_extraction.text\n",
    "    features : feature matrix returned from tfidf.fit_transform.to_array()\n",
    "    labels : class labels from input data\n",
    "    \"\"\"\n",
    "    from sklearn.feature_selection import chi2\n",
    "    import numpy as np\n",
    "    N = 5\n",
    "    for cond in list(set(labels)):\n",
    "      features_chi2 = chi2(features, labels == cond)\n",
    "      indices = np.argsort(features_chi2[0])\n",
    "      feature_names = np.array(tfidf.get_feature_names())[indices]\n",
    "      unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n",
    "      bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n",
    "      #trigrams = [v for v in feature_names if len(v.split(' ')) == 3]\n",
    "      print(\"# '{}':\".format(cond))\n",
    "      print(\"  . Most correlated unigrams:\\n. {}\".format('\\n. '.join(unigrams[-N:])))\n",
    "      print(\"  . Most correlated bigrams:\\n. {}\".format('\\n. '.join(bigrams[-N:])))\n",
    "      #print(\"  . Most correlated trigrams:\\n. {}\".format('\\n. '.join(trigrams[-N:])))\n",
    "      print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## How important is a word?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency, Inverse Document Frequency\n",
    "<img src=\"https://i.imgur.com/WkddTVo.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Two example \"documents\"\n",
    "example = [\n",
    "    'Perfectly cooked and seasoned', # A document we might classify as being about \"food\"\n",
    "    'Illustrious and a symbol of strength' # A document we would probably classify as not being about food - \"not food\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer # Import the \"TfidfVectorizer\" from scikit-learn\n",
    "import pandas as pd # Allows us to view data as a nicely formatted table!\n",
    "\n",
    "# Make a TF-IDF vectorizer object\n",
    "tfidf = TfidfVectorizer( \n",
    "    lowercase=True, # Make every word in our documents lowercase\n",
    "    stop_words='english' # Remove common words like \"a\", \"is\", \"they\", \"with\", etc.\n",
    ")\n",
    "\n",
    "# Transform the \"documents\" we have to a matrix of TF-IDF values\n",
    "features = tfidf.fit_transform(example).toarray()\n",
    "\n",
    "print(\n",
    "    # Create a dataframe from the matrix we got, with term labels\n",
    "    pd.DataFrame(\n",
    "        features, \n",
    "        columns = tfidf.get_feature_names() # Gets the names of the features from the tfidf object\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Let's put on our math hats\n",
    "\n",
    "Why does the word \"illustrious\" have a TF-IDF value of 0.57735?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "num_documents = 2 # We have two documents in our example\n",
    "num_words = 3 # There are three unique/meaningful words in each document (does not include the meaningless \"stopwords\")\n",
    "term_frequency = 1/num_words # The \"term frequency\" for each term in each document, given our example\n",
    "inverse_document_frequency = np.log(num_documents/1)+1 # The \"inverse document frequency\" for each term in each document\n",
    "\n",
    "# Note: These values are only accurate to the example we have given, \n",
    "# because each word is unique in each document.\n",
    "\n",
    "tf_idf = (1/3)*(np.log(2)+1)\n",
    "print(tf_idf) # Should equal about ~.56 - which is basically what we got back before, minus some \"smoothing\"!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Okay, _cool math dude_, so now we have a huge matrix. \n",
    "### So, how do we use it to make classifications?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# What the f@!k is Multinomial Naive Bayes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Enter Bayes Rule\n",
    "<img src=\"https://miro.medium.com/max/512/0*EfYTXtTJ9X-Ua9Nh.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## From our first (small) example\n",
    "<img src=\"https://i.imgur.com/WnKCeD3.png\" width=\"80%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Food or not food?\n",
    "<img src=\"https://i.imgur.com/a83Evsd.png\" width=\"55%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We're going to need some training data..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src=\"https://i.imgur.com/qXcsZPi.png\" width=\"50%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Let's do some classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Get the classifications we came up with!\n",
    "descriptions = pd.read_csv('iPhone Listing Descriptions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# What's the distribution of the condition classes we came up with?\n",
    "descriptions.Condition.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    min_df=.05, \n",
    "    lowercase=True,\n",
    "    ngram_range=(1, 2), # Consider both one word and two word combinations\n",
    "    stop_words='english'\n",
    ")\n",
    "\n",
    "features = tfidf.fit_transform(descriptions.ItemDescription).toarray()\n",
    "labels = descriptions.Condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Get most associated words with each condition category \n",
    "most_associated_words(tfidf, features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Train, train, train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split # Split a dataset into training and test datasets\n",
    "from sklearn.feature_extraction.text import CountVectorizer # Make a matrix of word counts\n",
    "from sklearn.feature_extraction.text import TfidfTransformer # Transform a matrix of word counts into TF-IDF values\n",
    "from sklearn.naive_bayes import MultinomialNB # Make a Multinomial Naive Bayes model\n",
    "\n",
    "# Make training and testing datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    descriptions['ItemDescription'], \n",
    "    descriptions['Condition'], \n",
    "    random_state = 0\n",
    ")\n",
    "\n",
    "# Transform our training data into word counts\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "\n",
    "# ..and then TF-IDF values\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "# Train the Multinomial Naive Bayes model!\n",
    "clf = MultinomialNB().fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Classify the real data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# We need some value data!\n",
    "iphone_listings = pd.read_csv('iPhone Listings.csv', error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# What does the data look like?\n",
    "iphone_listings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    # Use the model we trained to predict labels!\n",
    "    clf.predict(\n",
    "        # Transform the listing descriptions into a matrix of counts\n",
    "        count_vect.transform(\n",
    "            iphone_listings.head(100).ItemDescription.tolist()\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a new column in the real data with the predicted Condition\n",
    "iphone_listings['Condition'] = clf.predict(\n",
    "        # Transform the listing descriptions into a matrix of counts\n",
    "        count_vect.transform(\n",
    "            iphone_listings.ItemDescription.tolist()\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many of each class did we predict?\n",
    "iphone_listings.Condition.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's try valuing a used iPhone\n",
    "\n",
    "How about the:\n",
    "\n",
    " - Model: iPhone X\n",
    " - Storage Capacity: 256 GB\n",
    " - Color: Space Gray\n",
    " - Carrier: AT&T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How we might describe our iPhone\n",
    "iphone_listing_description = \"Very good - I've only had it since June. No scratches anywhere! It's been in a screen protector so you know it has to be in great condition!!!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the iPhone listing description we made\n",
    "prediction = clf.predict(\n",
    "    # Transform the listing descriptions into a matrix of counts\n",
    "    count_vect.transform(\n",
    "        [iphone_listing_description]\n",
    "    )\n",
    ")\n",
    "\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does the data look like, again?\n",
    "iphone_listings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset the listings to the following conditions\n",
    "iphone_matches = iphone_listings[\n",
    "    (iphone_listings.ModelName == 'iPhone X')\n",
    "    & (iphone_listings.Carrier == 'at&t')\n",
    "    & (iphone_listings.Storage == '256 GB')\n",
    "    & (iphone_listings.Color == 'space gray')\n",
    "    & (iphone_listings.Condition == int(prediction))\n",
    "]\n",
    "\n",
    "iphone_matches.Value.mean() # Get the \"Value\" column from the dataframe and then get the mean of the column"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
